{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b404e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66c5393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6245f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from catboost import CatBoostRegressor\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a786c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, HuberRegressor, Lasso, BayesianRidge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import GradientBoostingRegressor, BaggingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.svm import SVR\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from sklearn.base import BaseEstimator, RegressorMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfc1df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model training, base models\n",
    "train_processed = pd.read_csv('train_processed.csv')\n",
    "test_processed = pd.read_csv('test_processed.csv')\n",
    "\n",
    "# define features and target variable\n",
    "X = train_processed.drop(['SalePrice'], axis=1)\n",
    "y = train_processed['SalePrice']  # log transformed\n",
    "X_test = test_processed.copy()\n",
    "test = pd.read_csv('test.csv')\n",
    "test_ID = test['Id']  # save test IDs for submission\n",
    "\n",
    "# split train and validation sets,这里需要分层划分,以避免famd出现问题\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed8477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 默认参数基模型训练（使用降维后数据）\n",
    "# Random Forest\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_val)\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_val, rf_pred))\n",
    "print(f\"RandomForest RMSE (default): {rf_rmse:.5f}\")\n",
    "\n",
    "# XGBoost\n",
    "xgb = XGBRegressor(random_state=42)\n",
    "xgb.fit(X_train, y_train)\n",
    "xgb_pred = xgb.predict(X_val)\n",
    "xgb_rmse = np.sqrt(mean_squared_error(y_val, xgb_pred))\n",
    "print(f\"XGBoost RMSE (default): {xgb_rmse:.5f}\")\n",
    "\n",
    "# LightGBM\n",
    "lgb = LGBMRegressor(random_state=42,verbose=-1)\n",
    "lgb.fit(X_train, y_train)\n",
    "lgb_pred = lgb.predict(X_val)\n",
    "lgb_rmse = np.sqrt(mean_squared_error(y_val, lgb_pred))\n",
    "print(f\"LightGBM RMSE (default): {lgb_rmse:.5f}\")\n",
    "\n",
    "# CatBoost（降维后无需 categorical_features）\n",
    "# define categorical features for CatBoost\n",
    "categorical_features = ['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', \n",
    "                       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', \n",
    "                       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', \n",
    "                       'Exterior2nd', 'MasVnrType', 'Foundation', 'BsmtFinType1', \n",
    "                       'BsmtFinType2', 'Heating', 'CentralAir', 'Electrical', 'GarageType', \n",
    "                       'GarageFinish', 'PavedDrive', 'Fence', 'MiscFeature', 'SaleType', \n",
    "                       'SaleCondition', 'Season']\n",
    "catboost = CatBoostRegressor(random_state=42, verbose=0, cat_features=categorical_features)\n",
    "catboost.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50)\n",
    "catboost_pred = catboost.predict(X_val)\n",
    "catboost_rmse = np.sqrt(mean_squared_error(y_val, catboost_pred))\n",
    "print(f\"CatBoost RMSE (default): {catboost_rmse:.5f}\")\n",
    "\n",
    "# enet\n",
    "enet = ElasticNet(random_state=42)\n",
    "enet.fit(X_train, y_train)\n",
    "enet_pred = enet.predict(X_val)\n",
    "enet_rmse = np.sqrt(mean_squared_error(y_val, enet_pred))\n",
    "print(f\"ElasticNet RMSE (default): {enet_rmse:.5f}\")\n",
    "\n",
    "# HuberRegressor\n",
    "huber = HuberRegressor()\n",
    "huber.fit(X_train, y_train)\n",
    "huber_pred = huber.predict(X_val)\n",
    "huber_rmse = np.sqrt(mean_squared_error(y_val, huber_pred))\n",
    "print(f\"HuberRegressor RMSE (default): {huber_rmse:.5f}\")\n",
    "\n",
    "# MLPRegressor\n",
    "mlp = MLPRegressor(random_state=42, max_iter=1000)\n",
    "mlp.fit(X_train, y_train)\n",
    "mlp_pred = mlp.predict(X_val)\n",
    "mlp_rmse = np.sqrt(mean_squared_error(y_val, mlp_pred))\n",
    "print(f\"MLPRegressor RMSE (default): {mlp_rmse:.5f}\")\n",
    "\n",
    "# KernelRidge\n",
    "kr = KernelRidge()\n",
    "kr.fit(X_train, y_train)\n",
    "kr_pred = kr.predict(X_val)\n",
    "kr_rmse = np.sqrt(mean_squared_error(y_val, kr_pred))\n",
    "print(f\"KernelRidge RMSE (default): {kr_rmse:.5f}\")\n",
    "\n",
    "# SVR\n",
    "svr = SVR()\n",
    "svr.fit(X_train, y_train)\n",
    "svr_pred = svr.predict(X_val)\n",
    "svr_rmse = np.sqrt(mean_squared_error(y_val, svr_pred))\n",
    "print(f\"SVR RMSE (default): {svr_rmse:.5f}\")\n",
    "\n",
    "# Ridge\n",
    "ridge = Ridge()\n",
    "ridge.fit(X_train, y_train)\n",
    "ridge_pred = ridge.predict(X_val)\n",
    "ridge_rmse = np.sqrt(mean_squared_error(y_val, ridge_pred))\n",
    "print(f\"Ridge RMSE (default): {ridge_rmse:.5f}\")\n",
    "\n",
    "# Lasso\n",
    "lasso = Lasso()\n",
    "lasso.fit(X_train, y_train)\n",
    "lasso_pred = lasso.predict(X_val)\n",
    "lasso_rmse = np.sqrt(mean_squared_error(y_val, lasso_pred))\n",
    "print(f\"Lasso RMSE (default): {lasso_rmse:.5f}\")\n",
    "\n",
    "# GBR\n",
    "gbr = GradientBoostingRegressor()\n",
    "gbr.fit(X_train, y_train)\n",
    "gbr_pred = gbr.predict(X_val)\n",
    "gbr_rmse = np.sqrt(mean_squared_error(y_val, gbr_pred))\n",
    "print(f\"GBR RMSE (default): {gbr_rmse:.5f}\")\n",
    "\n",
    "# bagging\n",
    "bagging = BaggingRegressor()\n",
    "bagging.fit(X_train, y_train)\n",
    "bagging_pred = bagging.predict(X_val)\n",
    "bagging_rmse = np.sqrt(mean_squared_error(y_val, bagging_pred))\n",
    "print(f\"Bagging RMSE (default): {bagging_rmse:.5f}\")\n",
    "\n",
    "# TabNet\n",
    "tabnet = TabNetRegressor()\n",
    "X_train_np = X_train.values\n",
    "y_train_np = y_train.values.reshape(-1,1)\n",
    "X_val_np = X_val.values\n",
    "tabnet.fit(X_train_np, y_train_np)\n",
    "tabnet_pred = tabnet.predict(X_val_np)\n",
    "tabnet_rmse = np.sqrt(mean_squared_error(y_val, tabnet_pred))\n",
    "print(f\"TabNet RMSE (default): {tabnet_rmse:.5f}\")\n",
    "\n",
    "\n",
    "# BayesianRidge\n",
    "bayesian_ridge = BayesianRidge()\n",
    "bayesian_ridge.fit(X_train, y_train)\n",
    "bayesian_ridge_pred = bayesian_ridge.predict(X_val)\n",
    "bayesian_ridge_rmse = np.sqrt(mean_squared_error(y_val, bayesian_ridge_pred))\n",
    "print(f\"BayesianRidge RMSE (default): {bayesian_ridge_rmse:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234bdbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c68c21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest optuna 调优\n",
    "def rf_objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 4),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = RandomForestRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_val)\n",
    "    return np.sqrt(mean_squared_error(y_val, pred))\n",
    "\n",
    "rf_study = optuna.create_study(direction='minimize')\n",
    "rf_study.optimize(rf_objective, n_trials=200)  # RF 调优较慢，减少 trials\n",
    "rf_best_params = rf_study.best_params\n",
    "print(\"Best RandomForest params (Optuna):\", rf_best_params)\n",
    "\n",
    "# 重训练 Random Forest\n",
    "rf_best = RandomForestRegressor(**rf_best_params, random_state=42)\n",
    "rf_best.fit(X_train, y_train)\n",
    "rf_best_pred = rf_best.predict(X_val)\n",
    "rf_best_rmse = np.sqrt(mean_squared_error(y_val, rf_best_pred))\n",
    "print(f\"Optuna tuned RandomForest RMSE (reduced): {rf_best_rmse:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96016703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 Optuna 调优（每个模型单独定义 objective）\n",
    "# XGBoost Optuna 调优\n",
    "def xgb_objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 7),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 5),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = XGBRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_val)\n",
    "    return np.sqrt(mean_squared_error(y_val, pred))\n",
    "\n",
    "xgb_study = optuna.create_study(direction='minimize')\n",
    "xgb_study.optimize(xgb_objective, n_trials=n_trials)\n",
    "xgb_best_params = xgb_study.best_params\n",
    "print(\"Best XGBoost params (Optuna):\", xgb_best_params)\n",
    "\n",
    "# 重训练 XGBoost\n",
    "xgb_best = XGBRegressor(**xgb_best_params, random_state=42)\n",
    "xgb_best.fit(X_train, y_train)\n",
    "xgb_best_pred = xgb_best.predict(X_val)\n",
    "xgb_best_rmse = np.sqrt(mean_squared_error(y_val, xgb_best_pred))\n",
    "print(f\"Optuna tuned XGBoost RMSE (reduced): {xgb_best_rmse:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89b2758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM Optuna 调优\n",
    "def lgb_objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 7),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 15, 63),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = LGBMRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_val)\n",
    "    return np.sqrt(mean_squared_error(y_val, pred))\n",
    "\n",
    "lgb_study = optuna.create_study(direction='minimize')\n",
    "lgb_study.optimize(lgb_objective, n_trials=n_trials)\n",
    "lgb_best_params = lgb_study.best_params\n",
    "print(\"Best LightGBM params (Optuna):\", lgb_best_params)\n",
    "\n",
    "# 重训练 LightGBM\n",
    "lgb_best = LGBMRegressor(**lgb_best_params, random_state=42,verbose=-1)\n",
    "lgb_best.fit(X_train, y_train)\n",
    "lgb_best_pred = lgb_best.predict(X_val)\n",
    "lgb_best_rmse = np.sqrt(mean_squared_error(y_val, lgb_best_pred))\n",
    "print(f\"Optuna tuned LightGBM RMSE (reduced): {lgb_best_rmse:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b7cbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost Optuna 调优\n",
    "def catboost_objective(trial):\n",
    "    params = {\n",
    "        'iterations': 500,  # 初始小值，之后可增大\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'l2_leaf_reg': trial.suggest_int('l2_leaf_reg', 1, 7),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 128),\n",
    "        'random_state': 42,\n",
    "        'verbose': 0\n",
    "    }\n",
    "    model = CatBoostRegressor(**params)\n",
    "    model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50)\n",
    "    pred = model.predict(X_val)\n",
    "    return np.sqrt(mean_squared_error(y_val, pred))\n",
    "\n",
    "catboost_study = optuna.create_study(direction='minimize')\n",
    "catboost_study.optimize(catboost_objective, n_trials=n_trials)\n",
    "catboost_best_params = catboost_study.best_params\n",
    "catboost_best_params['iterations'] = 5000  # 增大 iterations 以利用最佳参数\n",
    "print(\"Best CatBoost params (Optuna):\", catboost_best_params)\n",
    "\n",
    "# 重训练 CatBoost\n",
    "catboost_best = CatBoostRegressor(**catboost_best_params, random_state=42, verbose=100)\n",
    "catboost_best.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=200)\n",
    "catboost_best_pred = catboost_best.predict(X_val)\n",
    "catboost_best_rmse = np.sqrt(mean_squared_error(y_val, catboost_best_pred))\n",
    "print(f\"Optuna tuned CatBoost RMSE (reduced): {catboost_best_rmse:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823a2045",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ElasticNet 调优\n",
    "def enet_objective(trial):\n",
    "    params = {\n",
    "        'alpha': trial.suggest_float('alpha', 0.0001, 1.0, log=True),\n",
    "        'l1_ratio': trial.suggest_float('l1_ratio', 0.0, 1.0),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = ElasticNet(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_val)\n",
    "    return np.sqrt(mean_squared_error(y_val, pred))\n",
    "\n",
    "enet_study = optuna.create_study(direction='minimize')\n",
    "enet_study.optimize(enet_objective, n_trials=n_trials)\n",
    "enet_best_params = enet_study.best_params\n",
    "print(\"Best ElasticNet params (Optuna):\", enet_best_params)\n",
    "\n",
    "# 重训练 ElasticNet\n",
    "enet_best = ElasticNet(**enet_best_params, random_state=42)\n",
    "enet_best.fit(X_train, y_train)\n",
    "enet_best_pred = enet_best.predict(X_val)\n",
    "enet_best_rmse = np.sqrt(mean_squared_error(y_val, enet_best_pred))\n",
    "print(f\"Optuna tuned ElasticNet RMSE: {enet_best_rmse:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f605ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huber 调优\n",
    "def objective_huber(trial):\n",
    "    params = {\n",
    "        'alpha': trial.suggest_float('alpha', 1e-4, 0.1, log=True),\n",
    "        'epsilon': trial.suggest_float('epsilon', 1.0, 2.0),\n",
    "        'max_iter': trial.suggest_int('max_iter', 2000, 5000)\n",
    "    }\n",
    "    model = HuberRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "    return rmse\n",
    "\n",
    "study_huber = optuna.create_study(direction='minimize')\n",
    "study_huber.optimize(objective_huber, n_trials=n_trials)\n",
    "huber_best_params = study_huber.best_params\n",
    "print(\"best Huber params:\", study_huber.best_params)\n",
    "print(\"best Huber RMSE:\", study_huber.best_value)\n",
    "\n",
    "# retrain\n",
    "huber_best = HuberRegressor(**study_huber.best_params)\n",
    "huber_best.fit(X_train, y_train)\n",
    "huber_best_pred = huber_best.predict(X_val)\n",
    "huber_best_rmse = np.sqrt(mean_squared_error(y_val, huber_best_pred))\n",
    "print(f\"Tuned Huber RMSE: {huber_best_rmse:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2fdb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP 调优\n",
    "def objective_mlp(trial):\n",
    "    params = {\n",
    "        'hidden_layer_sizes': trial.suggest_categorical('hidden_layer_sizes', [(50,100,50), (50,100,200,100,50), \n",
    "                                                                               (50,100,200,300,200,100,50), (100, 100),\n",
    "                                                                               (100,200,100),(100,200,300,200,100),\n",
    "                                                                               (100,200,300,400,300,200,100)]),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-5, 1e-2, log=True),\n",
    "        'learning_rate_init': trial.suggest_float('learning_rate_init', 1e-4, 1e-2, log=True),\n",
    "        'max_iter': trial.suggest_int('max_iter', 200, 1000)\n",
    "    }\n",
    "    model = MLPRegressor(**params, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "    return rmse\n",
    "\n",
    "study_mlp = optuna.create_study(direction='minimize')\n",
    "study_mlp.optimize(objective_mlp, n_trials=n_trials)\n",
    "mlp_best_params = study_mlp.best_params\n",
    "print(\"Best MLP params:\", study_mlp.best_params)\n",
    "print(\"Best MLP RMSE:\", study_mlp.best_value)\n",
    "\n",
    "# retrain\n",
    "mlp_best = MLPRegressor(**study_mlp.best_params, random_state=42)\n",
    "mlp_best.fit(X_train, y_train)\n",
    "mlp_best_pred = mlp_best.predict(X_val)\n",
    "mlp_best_rmse = np.sqrt(mean_squared_error(y_val, mlp_best_pred))\n",
    "print(f\"Tuned MLP RMSE: {mlp_best_rmse:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12008f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  KernelRidge 调优\n",
    "def kr_objective(trial):\n",
    "    kernel = trial.suggest_categorical('kernel', ['rbf', 'linear'])\n",
    "    params = {\n",
    "        'alpha': trial.suggest_float('alpha', 0.1, 10.0, log=True),\n",
    "        'kernel': kernel\n",
    "    }\n",
    "    if kernel == 'rbf':\n",
    "        params['gamma'] = trial.suggest_float('gamma', 0.001, 0.1, log=True)\n",
    "    model = KernelRidge(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_val)\n",
    "    return np.sqrt(mean_squared_error(y_val, pred))\n",
    "\n",
    "kr_study = optuna.create_study(direction='minimize')\n",
    "kr_study.optimize(kr_objective, n_trials=n_trials)\n",
    "kr_best_params = kr_study.best_params\n",
    "print(\"Best KernelRidge params (Optuna):\", kr_best_params)\n",
    "\n",
    "# 重训练 KernelRidge\n",
    "kr_best = KernelRidge(**kr_best_params)\n",
    "kr_best.fit(X_train, y_train)\n",
    "kr_best_pred = kr_best.predict(X_val)\n",
    "kr_best_rmse = np.sqrt(mean_squared_error(y_val, kr_best_pred))\n",
    "print(f\"Optuna tuned KernelRidge RMSE: {kr_best_rmse:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eee532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVR 调优\n",
    "def svr_objective(trial):\n",
    "    params = {\n",
    "        'C': trial.suggest_float('C', 0.1, 100.0, log=True),\n",
    "        'epsilon': trial.suggest_float('epsilon', 0.01, 0.5),\n",
    "        'kernel': trial.suggest_categorical('kernel', ['rbf', 'linear'])\n",
    "    }\n",
    "    model = SVR(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_val)\n",
    "    return np.sqrt(mean_squared_error(y_val, pred))\n",
    "\n",
    "svr_study = optuna.create_study(direction='minimize')\n",
    "svr_study.optimize(svr_objective, n_trials=n_trials)\n",
    "svr_best_params = svr_study.best_params\n",
    "print(\"Best SVR params (Optuna):\", svr_best_params)\n",
    "\n",
    "# 重训练 SVR\n",
    "svr_best = SVR(**svr_best_params)\n",
    "svr_best.fit(X_train, y_train)\n",
    "svr_best_pred = svr_best.predict(X_val)\n",
    "svr_best_rmse = np.sqrt(mean_squared_error(y_val, svr_best_pred))\n",
    "print(f\"Optuna tuned SVR RMSE: {svr_best_rmse:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6a8401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna optimization for Ridge\n",
    "def objective_ridge(trial):\n",
    "    alpha = trial.suggest_float('alpha', 0.1, 10.0, log=True)\n",
    "    ridge = Ridge(alpha=alpha, random_state=42)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    pred = ridge.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "    return rmse\n",
    "\n",
    "# Optimize hyperparameters with Optuna\n",
    "study_ridge = optuna.create_study(direction='minimize')\n",
    "study_ridge.optimize(objective_ridge, n_trials=n_trials)\n",
    "print(\"Best Ridge params:\", study_ridge.best_params)\n",
    "print(\"Best Ridge RMSE:\", study_ridge.best_value)\n",
    "\n",
    "# retrain\n",
    "ridge_best = Ridge(**study_ridge.best_params, random_state=42)\n",
    "ridge_best.fit(X_train, y_train)\n",
    "ridge_best_pred = ridge_best.predict(X_val)\n",
    "ridge_best_rmse = np.sqrt(mean_squared_error(y_val, ridge_best_pred))\n",
    "print(f\"Tuned Ridge RMSE: {ridge_best_rmse:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4cdf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna optimization for Lasso\n",
    "def objective_lasso(trial):\n",
    "    alpha = trial.suggest_float('alpha', 0.0001, 0.1, log=True)\n",
    "    lasso = Lasso(alpha=alpha, random_state=42)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    pred = lasso.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "    return rmse\n",
    "\n",
    "# Optimize hyperparameters with Optuna\n",
    "study_lasso = optuna.create_study(direction='minimize')\n",
    "study_lasso.optimize(objective_lasso, n_trials=n_trials)\n",
    "print(\"Best Lasso params:\", study_lasso.best_params)\n",
    "print(\"Best Lasso RMSE:\", study_lasso.best_value)\n",
    "\n",
    "# retrain\n",
    "lasso_best = Lasso(**study_lasso.best_params, random_state=42)\n",
    "lasso_best.fit(X_train, y_train)\n",
    "lasso_best_pred = lasso_best.predict(X_val)\n",
    "lasso_best_rmse = np.sqrt(mean_squared_error(y_val, lasso_best_pred))\n",
    "print(f\"Tuned Lasso RMSE: {lasso_best_rmse:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a790adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna optimization for GradientBoosting\n",
    "def objective_gbr(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 7),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 4),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0)\n",
    "    }\n",
    "    gbr = GradientBoostingRegressor(**params, random_state=42)\n",
    "    gbr.fit(X_train, y_train)\n",
    "    pred = gbr.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "    return rmse\n",
    "\n",
    "# Optimize hyperparameters with Optuna\n",
    "study_gbr = optuna.create_study(direction='minimize')\n",
    "study_gbr.optimize(objective_gbr, n_trials=n_trials)\n",
    "print(\"Best GradientBoosting params:\", study_gbr.best_params)\n",
    "print(\"Best GradientBoosting RMSE:\", study_gbr.best_value)\n",
    "\n",
    "# retrain\n",
    "gbr_best = GradientBoostingRegressor(**study_gbr.best_params, random_state=42)\n",
    "gbr_best.fit(X_train, y_train)\n",
    "gbr_best_pred = gbr_best.predict(X_val)\n",
    "gbr_best_rmse = np.sqrt(mean_squared_error(y_val, gbr_best_pred))\n",
    "print(f\"Tuned GradientBoosting RMSE: {gbr_best_rmse:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c568e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna optimization for Bagging\n",
    "def objective_br(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'max_samples': trial.suggest_float('max_samples', 0.5, 1.0),\n",
    "        'max_features': trial.suggest_float('max_features', 0.5, 1.0)\n",
    "    }\n",
    "    bagging = BaggingRegressor(**params, random_state=42)\n",
    "    bagging.fit(X_train, y_train)\n",
    "    pred = bagging.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "    return rmse\n",
    "\n",
    "# Optimize hyperparameters with Optuna\n",
    "study_bagging = optuna.create_study(direction='minimize')\n",
    "study_bagging.optimize(objective_br, n_trials=n_trials)\n",
    "print(\"Best Bagging params:\", study_bagging.best_params)\n",
    "print(\"Best Bagging RMSE:\", study_bagging.best_value)\n",
    "\n",
    "# retrain\n",
    "bagging_best = BaggingRegressor(**study_bagging.best_params, random_state=42)\n",
    "bagging_best.fit(X_train, y_train)\n",
    "bagging_best_pred = bagging_best.predict(X_val)\n",
    "bagging_best_rmse = np.sqrt(mean_squared_error(y_val, bagging_best_pred))\n",
    "print(f\"Tuned Bagging RMSE: {bagging_best_rmse:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82acfa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna optimization for TabNet\n",
    "def objective_tabnet(trial):\n",
    "    params = {\n",
    "        'n_d': trial.suggest_int('n_d', 8, 64),\n",
    "        'n_a': trial.suggest_int('n_a', 8, 64),\n",
    "        'n_steps': trial.suggest_int('n_steps', 3, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 1.0, 2.0),\n",
    "        'lambda_sparse': trial.suggest_float('lambda_sparse', 0.0001, 0.1, log=True)\n",
    "    }\n",
    "    tabnet = TabNetRegressor(**params, seed=42)\n",
    "    tabnet.fit(\n",
    "        X_train.values, y_train.values.reshape(-1, 1),\n",
    "        eval_set=[(X_val.values, y_val.values.reshape(-1, 1))],\n",
    "        eval_metric=['rmse'],\n",
    "        max_epochs=100,\n",
    "        patience=10,\n",
    "        batch_size=1024,\n",
    "        virtual_batch_size=128\n",
    "    )\n",
    "    pred = tabnet.predict(X_val.values).flatten()\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "    return rmse\n",
    "\n",
    "# Optimize hyperparameters with Optuna\n",
    "study_tabnet = optuna.create_study(direction='minimize')\n",
    "study_tabnet.optimize(objective_tabnet, n_trials=n_trials)\n",
    "print(\"Best TabNet params:\", study_tabnet.best_params)\n",
    "print(\"Best TabNet RMSE:\", study_tabnet.best_value)\n",
    "\n",
    "# retrain tabnet\n",
    "tabnet_best = TabNetRegressor(**study_tabnet.best_params, seed=42)\n",
    "tabnet_best.fit(\n",
    "    X_train.values, y_train.values.reshape(-1, 1),\n",
    "    eval_set=[(X_val.values, y_val.values.reshape(-1, 1))],\n",
    "    eval_metric=['rmse'],\n",
    "    max_epochs=100,\n",
    "    patience=10,\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=128\n",
    ")\n",
    "tabnet_best_pred = tabnet_best.predict(X_val.values).flatten()\n",
    "tabnet_best_rmse = np.sqrt(mean_squared_error(y_val, tabnet_best_pred))\n",
    "print(f\"Tuned TabNet RMSE: {tabnet_best_rmse:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de86ac47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna optimization for bayesian ridge\n",
    "def objective_br(trial):\n",
    "    params = {\n",
    "        'alpha_1': trial.suggest_float('alpha_1', 1e-6, 1e-1, log=True),\n",
    "        'alpha_2': trial.suggest_float('alpha_2', 1e-6, 1e-1, log=True),\n",
    "        'lambda_1': trial.suggest_float('lambda_1', 1e-6, 1e-1, log=True),\n",
    "        'lambda_2': trial.suggest_float('lambda_2', 1e-6, 1e-1, log=True)\n",
    "    }\n",
    "    bayesian_ridge = BayesianRidge(**params)\n",
    "    bayesian_ridge.fit(X_train, y_train)\n",
    "    pred = bayesian_ridge.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "    return rmse\n",
    "\n",
    "# Optimize hyperparameters with Optuna\n",
    "study_bayesian = optuna.create_study(direction='minimize')\n",
    "study_bayesian.optimize(objective_br, n_trials=n_trials)\n",
    "print(\"Best Bayesian params:\", study_bayesian.best_params)\n",
    "print(\"Best Bayesian RMSE:\", study_bayesian.best_value)\n",
    "\n",
    "# retrain\n",
    "bayesian_best = BayesianRidge(**study_bayesian.best_params)\n",
    "bayesian_best.fit(X_train, y_train)\n",
    "bayesian_best_pred = bayesian_best.predict(X_val)\n",
    "bayesian_best_rmse = np.sqrt(mean_squared_error(y_val, bayesian_best_pred))\n",
    "print(f\"Tuned Bayesian RMSE: {bayesian_best_rmse:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a7f838",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"1,best RF rmse:{rf_best_rmse}\")\n",
    "print(f\"2,best XGBoost rmse:{xgb_best_rmse}\")\n",
    "print(f\"3,best LightGBM rmse:{lgb_best_rmse}\")\n",
    "print(f\"4,best CatBoost rmse:{catboost_best_rmse}\")\n",
    "print(f\"5,best ElasticNet rmse:{enet_best_rmse}\")\n",
    "print(f\"6,best Huber rmse:{huber_best_rmse}\")\n",
    "print(f\"7,best MLP rmse:{mlp_best_rmse}\")\n",
    "print(f\"8,best kernelRidge rmse: {kr_best_rmse}\")\n",
    "print(f\"9,best SVR rmse:{svr_best_rmse}\")\n",
    "print(f\"10,best ridge rmse:{ridge_best_rmse}\")\n",
    "print(f\"11,best lasso rmse:{lasso_best_rmse}\")\n",
    "print(f\"12,best gbr rmse: {gbr_best_rmse}\")\n",
    "print(f\"13,best bagging rmse: {bagging_best_rmse}\")\n",
    "print(f\"14,best tabnet rmse: {tabnet_best_rmse}\")\n",
    "print(f\"15,best bayesian rmse: {bayesian_best_rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379a58fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # feature importance visualization\n",
    "# # Random Forest feature importance\n",
    "# rf_importance = pd.Series(rf_best.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# rf_importance[:30].plot(kind='bar')\n",
    "# plt.title('Random Forest top 10 feature importance')\n",
    "# plt.show()\n",
    "\n",
    "# # # XGBoost feature importance\n",
    "# # xgb_importance = pd.Series(xgb_best.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "# # plt.figure(figsize=(10, 6))\n",
    "# # xgb_importance[:30].plot(kind='bar')\n",
    "# # plt.title('XGBoost top 10 feature importance')\n",
    "# # plt.show()\n",
    "\n",
    "# # LightGBM feature importance\n",
    "# lgb_importance = pd.Series(lgb_best.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# lgb_importance[:30].plot(kind='bar')\n",
    "# plt.title('LightGBM top 10 feature importance')\n",
    "# plt.show()\n",
    "\n",
    "# # CatBoost feature importance\n",
    "# catboost_importance = pd.Series(catboost_best.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# catboost_importance[:30].plot(kind='bar')\n",
    "# plt.title('CatBoost top 10 feature importance')\n",
    "# plt.show()\n",
    "\n",
    "# # ElasticNet feature importance\n",
    "# enet_importance = pd.Series(np.abs(enet_best.coef_), index=X.columns).sort_values(ascending=False)\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# enet_importance[:30].plot(kind='bar')\n",
    "# plt.title('ElasticNet top 10 feature importance')\n",
    "# plt.show()\n",
    "\n",
    "# # Huber feature importance\n",
    "# huber_importance = pd.Series(np.abs(huber_best.coef_), index=X.columns).sort_values(ascending=False)\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# huber_importance[:30].plot(kind='bar')\n",
    "# plt.title('Huber top 10 feature importance')\n",
    "# plt.show()\n",
    "\n",
    "# # MLP feature importance\n",
    "# perm_importance = permutation_importance(mlp_best, X_val, y_val, n_repeats=10, random_state=42)\n",
    "# mlp_importance = pd.Series(perm_importance.importances_mean, index=X.columns).sort_values(ascending=False)\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# mlp_importance[:30].plot(kind='bar')\n",
    "# plt.title('MLP top 10 feature importance')\n",
    "# plt.show()\n",
    "\n",
    "# # KernelRidge feature importance, no feature importance available\n",
    "\n",
    "# # SVR feature importance, no feature importance available\n",
    "\n",
    "# # # Feature importance for GradientBoosting\n",
    "# # gbr_importance = pd.Series(gbr_best.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "# # plt.figure(figsize=(10, 6))\n",
    "# # gbr_importance[:10].plot(kind='bar')\n",
    "# # plt.title('GradientBoosting Top 10 Feature Importance')\n",
    "# # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21394034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Random Forest cross-validation\n",
    "# rf_cv_scores = cross_val_score(rf_best, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "# print(f\"Random Forest cross-validation RMSE: {-rf_cv_scores.mean():.5f} (+/- {rf_cv_scores.std() * 2:.5f})\")\n",
    "\n",
    "# # # XGBoost cross-validation\n",
    "# # xgb_cv_scores = cross_val_score(xgb_best, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "# # print(f\"XGBoost cross-validation RMSE: {-xgb_cv_scores.mean():.5f} (+/- {xgb_cv_scores.std() * 2:.5f})\")\n",
    "\n",
    "# # LightGBM cross-validation\n",
    "# lgb_cv_scores = cross_val_score(lgb_best, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "# print(f\"LightGBM cross-validation RMSE: {-lgb_cv_scores.mean():.5f} (+/- {lgb_cv_scores.std() * 2:.5f})\")\n",
    "\n",
    "# # CatBoost cross-validation\n",
    "# catboost_cv_scores = cross_val_score(catboost_best, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "# print(f\"CatBoost cross-validation RMSE: {-catboost_cv_scores.mean():.5f} (+/- {catboost_cv_scores.std() * 2:.5f})\")\n",
    "\n",
    "# # ElasticNet cross-validation\n",
    "# enet_cv_scores = cross_val_score(enet_best, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "# print(f\"ElasticNet cross-validation RMSE: {-enet_cv_scores.mean():.5f} (+/- {enet_cv_scores.std() * 2:.5f})\")\n",
    "\n",
    "# # Huber cross-validation\n",
    "# huber_cv_scores = cross_val_score(huber_best, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "# print(f\"Huber cross-validation RMSE: {-huber_cv_scores.mean():.5f} (+/- {huber_cv_scores.std() * 2:.5f})\")\n",
    "\n",
    "# # MLP cross-validation\n",
    "# mlp_cv_scores = cross_val_score(mlp_best, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "# print(f\"MLP cross-validation RMSE: {-mlp_cv_scores.mean():.5f} (+/- {mlp_cv_scores.std() * 2:.5f})\")\n",
    "\n",
    "# # # KernelRidge cross-validation\n",
    "# # kr_cv_scores = cross_val_score(kr_best, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "# # print(f\"KernelRidge cross-validation RMSE: {-kr_cv_scores.mean():.5f} (+/- {kr_cv_scores.std() * 2:.5f})\")\n",
    "\n",
    "# # # SVR cross-validation\n",
    "# # svr_cv_scores = cross_val_score(svr_best, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "# # print(f\"SVR cross-validation RMSE: {-svr_cv_scores.mean():.5f} (+/- {svr_cv_scores.std() * 2:.5f})\")\n",
    "\n",
    "# # # Cross-validation\n",
    "# # ridge_cv_scores = cross_val_score(ridge_best, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "# # print(f\"Ridge CV RMSE: {-ridge_cv_scores.mean():.5f} (+/- {ridge_cv_scores.std() * 2:.5f})\")\n",
    "\n",
    "# # lasso_cv_scores = cross_val_score(lasso_best, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "# # print(f\"Lasso CV RMSE: {-lasso_cv_scores.mean():.5f} (+/- {lasso_cv_scores.std() * 2:.5f})\")\n",
    "\n",
    "# # gbr_cv_scores = cross_val_score(gbr_best, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "# # print(f\"GradientBoosting CV RMSE: {-gbr_cv_scores.mean():.5f} (+/- {gbr_cv_scores.std() * 2:.5f})\")\n",
    "\n",
    "# # br_cv_scores = cross_val_score(br_best, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "# # print(f\"Bagging CV RMSE: {-br_cv_scores.mean():.5f} (+/- {br_cv_scores.std() * 2:.5f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4feafa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set\n",
    "rf_test_pred = rf_best.predict(X_test)\n",
    "lgb_test_pred = lgb_best.predict(X_test)\n",
    "enet_best_pred = enet_best.predict(X_test)\n",
    "catboost_test_pred = catboost_best.predict(X_test)\n",
    "huber_best_pred = huber_best.predict(X_test)\n",
    "mlp_best_pred = mlp_best.predict(X_test)\n",
    "\n",
    "# simple ensemble (average predictions)\n",
    "final_pred = (catboost_test_pred + rf_test_pred + lgb_test_pred + \n",
    "              enet_best_pred + huber_best_pred + mlp_best_pred) / 6\n",
    "\n",
    "# expm1 transformation to reverse log transformation\n",
    "final_pred = np.expm1(final_pred)\n",
    "\n",
    "# save submission file\n",
    "submission = pd.DataFrame({'Id': test_ID, 'SalePrice': final_pred})\n",
    "submission.to_csv('submission_baseline_6Models.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1725a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define base models for stacking\n",
    "base_models = [\n",
    "    ('rf', RandomForestRegressor(**rf_best_params, random_state=42)),\n",
    "    ('lgb', LGBMRegressor(**lgb_best_params, random_state=42, verbose=-1)),\n",
    "    ('catboost', CatBoostRegressor(**catboost_best_params, random_state=42, verbose=0)),\n",
    "    ('enet', ElasticNet(**enet_best_params, random_state=42)),\n",
    "    ('huber', HuberRegressor(**huber_best_params)),\n",
    "    ('mlp', MLPRegressor(**mlp_best_params, random_state=42))\n",
    "]\n",
    "\n",
    "# define meta learner\n",
    "meta_learner = Ridge()\n",
    "# first try use normal parameters for LGBMRegressor\n",
    "# meta_learner = LGBMRegressor(n_estimators=100, learning_rate=0.05, max_depth=3, \n",
    "#                             num_leaves=15, random_state=42)\n",
    "# initia Stacking\n",
    "stacking_model = StackingRegressor(estimators=base_models, final_estimator=meta_learner, cv=5)\n",
    "\n",
    "# train Stacking model\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# validate Stacking model\n",
    "stacking_pred = stacking_model.predict(X_val)\n",
    "stacking_rmse = np.sqrt(mean_squared_error(y_val, stacking_pred))\n",
    "print(f\"Stacking RMSE: {stacking_rmse:.5f}\")\n",
    "\n",
    "# cross-validation\n",
    "stacking_cv_scores = cross_val_score(stacking_model, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "print(f\"Stacking cross-validation RMSE: {-stacking_cv_scores.mean():.5f} (+/- {stacking_cv_scores.std() * 2:.5f})\")\n",
    "\n",
    "# predict on test set\n",
    "stacking_test_pred = stacking_model.predict(X_test)\n",
    "stacking_test_pred = np.expm1(stacking_test_pred)\n",
    "\n",
    "# save submission file\n",
    "submission_stacking = pd.DataFrame({'Id': test_ID, 'SalePrice': stacking_test_pred})\n",
    "submission_stacking.to_csv('submission_stacking_6Models.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
