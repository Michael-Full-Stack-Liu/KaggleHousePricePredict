{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1b404e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6245f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2c5757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9989442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdfc1df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try DNN\n",
    "train_processed = pd.read_csv('train_processed.csv')\n",
    "test_processed = pd.read_csv('test_processed.csv')\n",
    "test_id = pd.read_csv('test.csv')['Id']\n",
    "\n",
    "# define features and target variable\n",
    "X = train_processed.drop(['SalePrice'], axis=1).to_numpy()\n",
    "y = train_processed['SalePrice'].to_numpy()  # log transformed\n",
    "X_test = test_processed.copy().to_numpy()\n",
    "\n",
    "# split train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89462f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch ÂÆö‰πâÊï∞ÊçÆÈõÜÁ±ª,Â∞ÜÁâπÂæÅÂíåÁõÆÊ†áËΩ¨Êç¢‰∏∫Âº†Èáè\n",
    "class HousePriceDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32) if y is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is not None:\n",
    "            return self.X[idx], self.y[idx]\n",
    "        return self.X[idx]\n",
    "\n",
    "# ÂàõÂª∫ DataLoader\n",
    "train_dataset = HousePriceDataset(X_train, y_train)\n",
    "val_dataset = HousePriceDataset(X_val, y_val)\n",
    "test_dataset = HousePriceDataset(X_test)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbe576a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type       | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | model     | Sequential | 1.9 M  | train\n",
      "1 | criterion | MSELoss    | 0      | train\n",
      "-------------------------------------------------\n",
      "1.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 M     Total params\n",
      "7.405     Total estimated model params size (MB)\n",
      "31        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:00<00:00, 44.87it/s, v_num=55, train_loss=1.650, val_rmse=0.220]\n",
      "Testing DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 46/46 [00:00<00:00, 228.49it/s]\n"
     ]
    }
   ],
   "source": [
    "class HousePriceDNN(L.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dims=[2048, 1024, 512, 256, 128, 64, 32], dropout=0.2):\n",
    "        super(HousePriceDNN, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, dim),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(dim),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = dim\n",
    "        layers.append(nn.Linear(prev_dim, 1))  # ÂõûÂΩíËæìÂá∫Â±Ç\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.criterion(y_pred, y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.criterion(y_pred, y)\n",
    "        rmse = torch.sqrt(loss)\n",
    "        self.log('val_rmse', rmse, prog_bar=True)\n",
    "        return rmse\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        y_pred = self(x)\n",
    "        self.predictions.append(y_pred.cpu().numpy())\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_rmse'\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def on_test_epoch_start(self):\n",
    "        self.predictions = []\n",
    "\n",
    "# ÂàùÂßãÂåñÊ®°Âûã\n",
    "input_dim = X_train.shape[1]\n",
    "model = HousePriceDNN(input_dim)\n",
    "\n",
    "# ËÆæÁΩÆËÆ≠ÁªÉÂô®\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=200,\n",
    "    accelerator='auto',  # Ëá™Âä®ÈÄâÊã© GPU/CPU\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='val_rmse', patience=20, mode='min'),\n",
    "        ModelCheckpoint(monitor='val_rmse', save_top_k=1, mode='min')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ËÆ≠ÁªÉÊ®°Âûã\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "# ÊµãËØïÈõÜÈ¢ÑÊµã\n",
    "trainer.test(model, test_loader)\n",
    "test_pred = np.concatenate(model.predictions)\n",
    "test_pred = np.expm1(test_pred)  # ÈÄÜÂØπÊï∞ÂèòÊç¢\n",
    "\n",
    "# ÁîüÊàêÊèê‰∫§Êñá‰ª∂\n",
    "submission = pd.DataFrame({'Id': test_id, 'SalePrice': test_pred})\n",
    "submission.to_csv('submission_dnn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50457070",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optuna' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m     trainer.fit(model, train_loader, val_loader)\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer.callback_metrics[\u001b[33m'\u001b[39m\u001b[33mval_rmse\u001b[39m\u001b[33m'\u001b[39m].item()\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m study = \u001b[43moptuna\u001b[49m.create_study(direction=\u001b[33m'\u001b[39m\u001b[33mminimize\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     15\u001b[39m study.optimize(objective, n_trials=\u001b[32m50\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'optuna' is not defined"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    hidden_dims = [\n",
    "        trial.suggest_int('layer1', 128, 512),\n",
    "        trial.suggest_int('layer2', 64, 512),\n",
    "        trial.suggest_int('layer3', 32, 512),\n",
    "        trial.suggest_int('layer4', 16, 512)\n",
    "    ]\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "    model = HousePriceDNN(input_dim, hidden_dims, dropout)\n",
    "    trainer = L.Trainer(max_epochs=50, callbacks=[EarlyStopping(monitor='val_rmse', patience=10)])\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    return trainer.callback_metrics['val_rmse'].item()\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90910d45",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'study' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Ê£ÄÊü•ÊúÄ‰Ω≥ÂèÇÊï∞\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m best_params = \u001b[43mstudy\u001b[49m.best_params\n\u001b[32m      3\u001b[39m best_hidden_sizes = [best_params[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mlayer\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[32m5\u001b[39m)]\n\u001b[32m      4\u001b[39m best_dropout = best_params[\u001b[33m'\u001b[39m\u001b[33mdropout\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'study' is not defined"
     ]
    }
   ],
   "source": [
    "# Ê£ÄÊü•ÊúÄ‰Ω≥ÂèÇÊï∞\n",
    "best_params = study.best_params\n",
    "best_hidden_sizes = [best_params[f'layer{i}'] for i in range(1, 5)]\n",
    "best_dropout = best_params['dropout']\n",
    "print(\"Best Parameters:\")\n",
    "print(f\"  Hidden Layer Sizes: {best_hidden_sizes}\")\n",
    "print(f\"  Dropout Rate: {best_dropout}\")\n",
    "print(f\"  Best RMSE from Optimization: {study.best_value:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e01140c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type       | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | model     | Sequential | 1.6 M  | train\n",
      "1 | criterion | MSELoss    | 0      | train\n",
      "-------------------------------------------------\n",
      "1.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 M     Total params\n",
      "6.205     Total estimated model params size (MB)\n",
      "15        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:00<00:00, 53.89it/s, v_num=106, train_loss=0.807, val_rmse=0.538]\n",
      "Final Validation RMSE after Training: 0.54575\n"
     ]
    }
   ],
   "source": [
    "# Áî®ÊúÄ‰Ω≥ÂèÇÊï∞ÈáçËÆ≠\n",
    "best_model = HousePriceDNN(input_dim, best_hidden_sizes, best_dropout)\n",
    "trainer = L.Trainer(max_epochs=500, callbacks=[EarlyStopping(monitor='val_rmse', patience=10)])\n",
    "trainer.fit(best_model, train_loader, val_loader)\n",
    "\n",
    "# Evaluate on validation set\n",
    "best_model.eval()\n",
    "val_preds = []\n",
    "val_true = []\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        x, y = batch\n",
    "        preds = best_model(x)\n",
    "        val_preds.extend(preds.cpu().numpy())\n",
    "        val_true.extend(y.cpu().numpy())\n",
    "\n",
    "# Calculate RMSE on validation set\n",
    "val_rmse = np.sqrt(mean_squared_error(val_true, val_preds))\n",
    "print(f\"Final Validation RMSE after Training: {val_rmse:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2789f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21394034",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for dimension 0 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m test_preds = np.zeros(\u001b[38;5;28mlen\u001b[39m(X_test))\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m fold, (train_idx, val_idx) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(kf.split(X)):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     train_data = HousePriceDataset(X[train_idx], \u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m     10\u001b[39m     val_data = HousePriceDataset(X[val_idx], y[val_idx])\n\u001b[32m     11\u001b[39m     train_loader = DataLoader(train_data, batch_size=\u001b[32m32\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mIndexError\u001b[39m: index 4 is out of bounds for dimension 0 with size 4"
     ]
    }
   ],
   "source": [
    "# DNN cross-validation\n",
    "# catboost_cv_scores = cross_val_score(catboost_best, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "# print(f\"CatBoost cross-validation RMSE: {-catboost_cv_scores.mean():.5f} (+/- {catboost_cv_scores.std() * 2:.5f})\")\n",
    "\n",
    "# Kfold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "test_preds = np.zeros(len(X_test))\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    train_data = HousePriceDataset(X[train_idx], y[train_idx])\n",
    "    val_data = HousePriceDataset(X[val_idx], y[val_idx])\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=32)\n",
    "    model = HousePriceDNN(input_dim)\n",
    "    trainer = pl.Trainer(max_epochs=100, callbacks=[pl.callbacks.EarlyStopping(monitor='val_rmse', patience=20)])\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    test_preds += np.expm1(np.concatenate(trainer.test(model, test_loader)))\n",
    "test_preds /= 5\n",
    "submission = pd.DataFrame({'Id': test_id, 'SalePrice': test_preds})\n",
    "submission.to_csv('submission_cv.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4feafa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set\n",
    "# xgb_test_pred = xgb_best.predict(X_test)\n",
    "# lgb_test_pred = lgb_best.predict(X_test)\n",
    "# catboost_test_pred = catboost_best.predict(X_test)\n",
    "\n",
    "# simple ensemble (average predictions)\n",
    "final_pred = test_pred\n",
    "\n",
    "# expm1 transformation to reverse log transformation\n",
    "final_pred = np.expm1(final_pred)\n",
    "\n",
    "# save submission file\n",
    "submission = pd.DataFrame({'Id': test_id, 'SalePrice': final_pred})\n",
    "submission.to_csv('submission_baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1725a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define base models for stacking\n",
    "base_models = [\n",
    "    ('rf', RandomForestRegressor(random_state=42)),\n",
    "    ('xgb', XGBRegressor(**xgb_grid.best_params_, random_state=42)),\n",
    "    ('lgb', LGBMRegressor(**lgb_grid.best_params_, random_state=42)),\n",
    "    ('catboost', CatBoostRegressor(**catboost_grid.best_params_, cat_features=categorical_features, random_state=42, verbose=0))\n",
    "]\n",
    "\n",
    "# define meta learner\n",
    "# meta_learner = Ridge()\n",
    "# first try use normal parameters for LGBMRegressor\n",
    "meta_learner = LGBMRegressor(n_estimators=100, learning_rate=0.05, max_depth=3, \n",
    "                              num_leaves=15, random_state=42)\n",
    "# initia Stacking\n",
    "stacking_model = StackingRegressor(estimators=base_models, final_estimator=meta_learner, cv=5)\n",
    "\n",
    "# train Stacking model\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# validate Stacking model\n",
    "stacking_pred = stacking_model.predict(X_val)\n",
    "stacking_rmse = np.sqrt(mean_squared_error(y_val, stacking_pred))\n",
    "print(f\"Stacking RMSE: {stacking_rmse:.5f}\")\n",
    "\n",
    "# cross-validation\n",
    "stacking_cv_scores = cross_val_score(stacking_model, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "print(f\"Stacking cross-validation RMSE: {-stacking_cv_scores.mean():.5f} (+/- {stacking_cv_scores.std() * 2:.5f})\")\n",
    "\n",
    "# predict on test set\n",
    "stacking_test_pred = stacking_model.predict(X_test)\n",
    "stacking_test_pred = np.expm1(stacking_test_pred)\n",
    "\n",
    "# save submission file\n",
    "submission_stacking = pd.DataFrame({'Id': test['Id'], 'SalePrice': stacking_test_pred})\n",
    "submission_stacking.to_csv('submission_stacking.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
