{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b404e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd2beb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43699da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6245f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a786c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4dec47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d637b8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.isnull().sum().sort_values(ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1660c34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.isnull().sum().sort_values(ascending=False).head(35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce55f2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data clean\n",
    "# missing values \n",
    "# high_missing_cols fill with 'None'\n",
    "high_missing_cols = ['PoolQC', 'MiscFeature', 'Alley', 'Fence']\n",
    "for col in high_missing_cols:\n",
    "    train[col] = train[col].fillna('None')\n",
    "    test[col] = test[col].fillna('None')\n",
    "\n",
    "# middle_missing_cols fill with 'None' except LotFrontage\n",
    "for col in ['MasVnrType', 'FireplaceQu']:\n",
    "    train[col] = train[col].fillna('None')\n",
    "    test[col] = test[col].fillna('None')\n",
    "\n",
    "# low missing（except MSZoning）\n",
    "zero_fill_cols = ['MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']\n",
    "for col in zero_fill_cols:\n",
    "    train[col] = train[col].fillna(0)\n",
    "    test[col] = test[col].fillna(0)\n",
    "#categories and garagecars,garagearea\n",
    "cat_cols = ['Electrical', 'Utilities', 'Functional', 'Exterior1st', 'Exterior2nd', 'SaleType', 'KitchenQual',\n",
    "            'BsmtExposure', 'BsmtCond', 'BsmtQual', 'BsmtFinType1', 'BsmtFinType2','GarageCars', 'GarageArea']\n",
    "for col in cat_cols:\n",
    "    train[col] = train[col].fillna(train[col].mode()[0])\n",
    "    test[col] = test[col].fillna(test[col].mode()[0])\n",
    "train['GarageYrBlt'] = train['GarageYrBlt'].fillna(0)\n",
    "test['GarageYrBlt'] = test['GarageYrBlt'].fillna(0)\n",
    "garage_cols = ['GarageQual', 'GarageFinish', 'GarageType', 'GarageCond']\n",
    "for col in garage_cols:\n",
    "    train[col] = train[col].fillna('None')\n",
    "    test[col] = test[col].fillna('None')\n",
    "\n",
    "\n",
    "# KNN fill LotFrontage and MSZoning\n",
    "knn_cols = ['LotFrontage', 'MSZoning']\n",
    "related_cols = ['GrLivArea', 'LotArea', 'OverallQual', 'Neighborhood']\n",
    "combined = pd.concat([train[knn_cols + related_cols], test[knn_cols + related_cols]], axis=0)\n",
    "label_encoders = {}\n",
    "for col in ['MSZoning', 'Neighborhood']:\n",
    "    le = LabelEncoder()\n",
    "    combined[col] = combined[col].fillna('Missing')\n",
    "    combined[col] = le.fit_transform(combined[col])\n",
    "    label_encoders[col] = le\n",
    "scaler = StandardScaler()\n",
    "num_cols = ['LotFrontage', 'GrLivArea', 'LotArea', 'OverallQual']\n",
    "combined[num_cols] = scaler.fit_transform(combined[num_cols])\n",
    "imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
    "combined_imputed = pd.DataFrame(imputer.fit_transform(combined), columns=combined.columns)\n",
    "combined_imputed[num_cols] = scaler.inverse_transform(combined_imputed[num_cols])\n",
    "combined_imputed['MSZoning'] = label_encoders['MSZoning'].inverse_transform(combined_imputed['MSZoning'].round().astype(int))\n",
    "combined_imputed['Neighborhood'] = label_encoders['Neighborhood'].inverse_transform(combined_imputed['Neighborhood'].round().astype(int))\n",
    "train[knn_cols] = combined_imputed.iloc[:train.shape[0], :][knn_cols]\n",
    "test[knn_cols] = combined_imputed.iloc[:test.shape[0], :][knn_cols]\n",
    "\n",
    "# validate missing values\n",
    "print(\"Train missing values:\\n\", train.isnull().sum().sort_values(ascending=False).head(5))\n",
    "print(\"Test missing values:\\n\", test.isnull().sum().sort_values(ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9787175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier removal\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='GrLivArea', y='SalePrice', hue='PoolQC', data=train)\n",
    "plt.title('GrLivArea vs SalePrice (Colored by PoolQC)')\n",
    "plt.show()\n",
    "train = train[train['GrLivArea'] < 4000]\n",
    "train = train[train['SalePrice'] < 700000]\n",
    "print(\"Train shape after outlier removal:\", train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da13d4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform categorical columns to string\n",
    "cat_cols = ['MSSubClass', 'OverallQual', 'OverallCond', 'PoolQC', 'MiscFeature', 'Alley', 'Fence', 'MSZoning']\n",
    "train[cat_cols] = train[cat_cols].astype(str)\n",
    "test[cat_cols] = test[cat_cols].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b162467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned data\n",
    "train.to_csv('train_cleaned_knn.csv', index=False)\n",
    "test.to_csv('test_cleaned_knn.csv', index=False)\n",
    "print(\"Train shape after cleaning:\", train.shape)\n",
    "print(\"Test shape after cleaning:\", test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e26ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate KNN imputation effect\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(train['LotFrontage'], kde=True, label='Train (KNN Imputed)')\n",
    "plt.title('LotFrontage Distribution After KNN Imputation')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='MSZoning', y='SalePrice', data=train)\n",
    "plt.title('MSZoning vs SalePrice After KNN Imputation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb67a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Feature engineering\n",
    "# load cleaned data\n",
    "train1 = pd.read_csv('train_cleaned_knn.csv',keep_default_na=False).set_index('Id') \n",
    "#keep_default_na=False to avoid NaN issues, important\n",
    "#set_index('Id') to exclude id column, important\n",
    "test1 = pd.read_csv('test_cleaned_knn.csv',keep_default_na=False).set_index('Id') \n",
    "#keep_default_na=False to avoid NaN issues, important\n",
    "#set_index('Id') to exclude id column, important\n",
    "\n",
    "# combine datasets for feature engineering consistency\n",
    "train1['is_train'] = 1\n",
    "test1['is_train'] = 0\n",
    "combined = pd.concat([train1.drop('SalePrice', axis=1), test1], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7974d64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. create new features\n",
    "# total area (above ground + basement)\n",
    "combined['TotalSF'] = combined['TotalBsmtSF'] + combined['1stFlrSF'] + combined['2ndFlrSF']\n",
    "\n",
    "# house age (year sold - year built)\n",
    "combined['HouseAge'] = combined['YrSold'] - combined['YearBuilt']\n",
    "\n",
    "# renewal age (year sold - year remodeled)\n",
    "combined['RemodAge'] = combined['YrSold'] - combined['YearRemodAdd']\n",
    "\n",
    "# total bathroom count (above ground + basement)\n",
    "combined['TotalBath'] = combined['FullBath'] + 0.5 * combined['HalfBath'] + \\\n",
    "                       combined['BsmtFullBath'] + 0.5 * combined['BsmtHalfBath']\n",
    "\n",
    "# total rooms (excluding bathrooms)\n",
    "combined['TotalRooms'] = combined['TotRmsAbvGrd'] + combined['BedroomAbvGr']\n",
    "\n",
    "# garage age (if no garage, fill 0)\n",
    "combined['GarageAge'] = combined['YrSold'] - combined['GarageYrBlt'].fillna(0)\n",
    "\n",
    "# swimming pool, fence, miscellaneous features\n",
    "combined['HasPool'] = combined['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n",
    "combined['HasFence'] = combined['Fence'].apply(lambda x: 1 if x != 'None' else 0)\n",
    "combined['HasMisc'] = combined['MiscFeature'].apply(lambda x: 1 if x != 'None' else 0)\n",
    "\n",
    "# additional features\n",
    "combined['SF_Qual'] = combined['TotalSF'] * combined['OverallQual']\n",
    "combined['LivArea_Bedroom'] = combined['GrLivArea'] * combined['BedroomAbvGr']\n",
    "\n",
    "# season feature\n",
    "def get_season(month):\n",
    "    if month in [3, 4, 5]: return 'Spring'\n",
    "    elif month in [6, 7, 8]: return 'Summer'\n",
    "    elif month in [9, 10, 11]: return 'Fall'\n",
    "    else: return 'Winter'\n",
    "combined['Season'] = combined['MoSold'].apply(get_season)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2326f520",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['FireplaceQu'].nunique()\n",
    "combined['FireplaceQu'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e17a26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['ExterQual'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736fd1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. categorical encoding\n",
    "# ordinal variable mapping\n",
    "ordinal_cols = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', \n",
    "                'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond', \n",
    "                'BsmtExposure', 'Functional','PoolQC']\n",
    "quality_map = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0, 'NA': 0 }\n",
    "bsmt_exposure_map = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1,  'NA': 0}\n",
    "functional_map = {'Typ': 8, 'Min1': 7, 'Min2': 6, 'Mod': 5, 'Maj1': 4, \n",
    "                  'Maj2': 3, 'Sev': 2, 'Sal': 1}\n",
    "PoolQC_map = {'Ex': 4, 'Gd': 3, 'TA': 2, 'Fa': 1, 'None': 0}\n",
    "for col in ordinal_cols:\n",
    "    if col == 'BsmtExposure':\n",
    "        combined[col] = combined[col].map(bsmt_exposure_map)\n",
    "    elif col == 'Functional':\n",
    "        combined[col] = combined[col].map(functional_map)\n",
    "    elif col == 'PoolQC':\n",
    "        combined[col] = combined[col].map(PoolQC_map)\n",
    "    else:\n",
    "        combined[col] = combined[col].map(quality_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3800485f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding \n",
    "nominal_cols = ['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', \n",
    "                'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', \n",
    "                'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', \n",
    "                'Exterior2nd', 'MasVnrType', 'Foundation', 'BsmtFinType1', \n",
    "                'BsmtFinType2', 'Heating', 'CentralAir', 'Electrical', 'GarageType', \n",
    "                'GarageFinish', 'PavedDrive', 'Fence', 'MiscFeature', 'SaleType', \n",
    "                'SaleCondition', 'Season']\n",
    "label_encoders = {}\n",
    "for col in nominal_cols:\n",
    "    le = LabelEncoder()\n",
    "    combined[col] = le.fit_transform(combined[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# encoding Neighborhood\n",
    "neighborhood_mean = train1.groupby('Neighborhood')['SalePrice'].mean()\n",
    "combined['Neighborhood_MeanPrice'] = combined['Neighborhood'].map(neighborhood_mean)\n",
    "combined['Neighborhood_MeanPrice'] = combined['Neighborhood_MeanPrice'].fillna(neighborhood_mean.mean())\n",
    "\n",
    "# 3. clustering\n",
    "cluster_features = ['TotalSF', 'OverallQual', 'GrLivArea']\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "combined['Cluster'] = kmeans.fit_predict(combined[cluster_features])\n",
    "\n",
    "# 4. delete low information columns\n",
    "# low_info_cols = ['LowQualFinSF', 'MiscVal', 'PoolArea']\n",
    "# combined = combined.drop(low_info_cols, axis=1)\n",
    "\n",
    "# 5. deal with skewness\n",
    "# log transform SalePrice\n",
    "train1['SalePrice'] = np.log1p(train1['SalePrice'])\n",
    "\n",
    "# transform skewed numeric features when skewness > 0.75\n",
    "numeric_cols = combined.select_dtypes(include=[np.number]).columns.drop(['is_train'])\n",
    "skewed_cols = combined[numeric_cols].apply(lambda x: x.skew()).sort_values(ascending=False)\n",
    "skewed_cols = skewed_cols[abs(skewed_cols) > 0.75].index\n",
    "for col in skewed_cols:\n",
    "    combined[col] = np.log1p(combined[col].clip(lower=0))\n",
    "\n",
    "# 6. standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "combined[numeric_cols] = scaler.fit_transform(combined[numeric_cols])\n",
    "\n",
    "# 7. separate train and test sets\n",
    "train_processed = combined[combined['is_train'] == 1].drop('is_train', axis=1)\n",
    "test_processed = combined[combined['is_train'] == 0].drop('is_train', axis=1)\n",
    "train_processed['SalePrice'] = train1['SalePrice']\n",
    "\n",
    "# 8. validate feature engineering\n",
    "print(\"New features created:\", ['TotalSF', 'HouseAge', 'RemodAge', 'TotalBath', \n",
    "                              'TotalRooms', 'GarageAge', 'HasPool', 'HasFence', \n",
    "                              'HasMisc', 'SF_Qual', 'LivArea_Bedroom', \n",
    "                              'Neighborhood_MeanPrice', 'Season', 'Cluster'])\n",
    "print(\"Train processed shape:\", train_processed.shape)\n",
    "print(\"Test processed shape:\", test_processed.shape)\n",
    "print(\"Missing values in train:\", train_processed.isnull().sum().max())\n",
    "print(\"Missing values in test:\", test_processed.isnull().sum().max())\n",
    "\n",
    "# save processed data\n",
    "train_processed.to_csv('train_processed.csv', index=False)\n",
    "test_processed.to_csv('test_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bdc470",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_processed.isnull().sum().sort_values(ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71d5449",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_processed.isnull().sum().sort_values(ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfc1df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model training, base models\n",
    "train_processed = pd.read_csv('train_processed.csv')\n",
    "test_processed = pd.read_csv('test_processed.csv')\n",
    "\n",
    "# define features and target variable\n",
    "X = train_processed.drop(['SalePrice'], axis=1)\n",
    "y = train_processed['SalePrice']  # log transformed\n",
    "X_test = test_processed.copy()\n",
    "\n",
    "# split train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26064e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default  base models\n",
    "# random forest\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_val)\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_val, rf_pred))\n",
    "print(f\"RandomForest RMSE: {rf_rmse:.5f}\")\n",
    "\n",
    "# XGBoost\n",
    "xgb = XGBRegressor(random_state=42)\n",
    "xgb.fit(X_train, y_train)\n",
    "xgb_pred = xgb.predict(X_val)\n",
    "xgb_rmse = np.sqrt(mean_squared_error(y_val, xgb_pred))\n",
    "print(f\"XGBoost RMSE: {xgb_rmse:.5f}\")\n",
    "\n",
    "# LightGBM\n",
    "lgb = LGBMRegressor(random_state=42)\n",
    "lgb.fit(X_train, y_train)\n",
    "lgb_pred = lgb.predict(X_val)\n",
    "lgb_rmse = np.sqrt(mean_squared_error(y_val, lgb_pred))\n",
    "print(f\"LightGBM RMSE: {lgb_rmse:.5f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96016703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost hyperparameter tuning\n",
    "# XGBoost parameter grid\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.7, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0],\n",
    "    'min_child_weight': [1, 3, 5]\n",
    "}\n",
    "\n",
    "# grid search for hyperparameter tuning\n",
    "xgb = XGBRegressor(random_state=42)\n",
    "xgb_grid = GridSearchCV(xgb, xgb_param_grid, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "\n",
    "# output best parameters and score\n",
    "print(\"best XGBoost params:\", xgb_grid.best_params_)\n",
    "print(\"best XGBoost RMSE:\", -xgb_grid.best_score_)\n",
    "\n",
    "# validate tuned XGBoost model\n",
    "xgb_best = xgb_grid.best_estimator_\n",
    "xgb_pred = xgb_best.predict(X_val)\n",
    "xgb_rmse = np.sqrt(mean_squared_error(y_val, xgb_pred))\n",
    "print(f\"Hyperparameter tuned XGBoost RMSE: {xgb_rmse:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89b2758",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lgbm hyperparameter tuning\n",
    "lgb_param_grid = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'num_leaves': [15, 31, 63],\n",
    "    'subsample': [0.7, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# grid search for hyperparameter tuning\n",
    "lgb = LGBMRegressor(random_state=42)\n",
    "lgb_grid = GridSearchCV(lgb, lgb_param_grid, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "lgb_grid.fit(X_train, y_train)\n",
    "\n",
    "# output best parameters and score\n",
    "print(\"Best LightGBM params:\", lgb_grid.best_params_)\n",
    "print(\"Best LightGBM RMSE:\", -lgb_grid.best_score_)\n",
    "\n",
    "# validate tuned XGBoost model\n",
    "lgb_best = lgb_grid.best_estimator_\n",
    "lgb_pred = lgb_best.predict(X_val)\n",
    "lgb_rmse = np.sqrt(mean_squared_error(y_val, lgb_pred))\n",
    "print(f\"Hyperparameter tuned LightGBM RMSE: {lgb_rmse:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379a58fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance visualization\n",
    "# XGBoost feature importance\n",
    "xgb_importance = pd.Series(xgb_best.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10, 6))\n",
    "xgb_importance[:10].plot(kind='bar')\n",
    "plt.title('XGBoost top 10 feature importance')\n",
    "plt.show()\n",
    "\n",
    "# LightGBM feature importance\n",
    "lgb_importance = pd.Series(lgb_best.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10, 6))\n",
    "lgb_importance[:10].plot(kind='bar')\n",
    "plt.title('LightGBM top 10 feature importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21394034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost cross-validation\n",
    "xgb_cv_scores = cross_val_score(xgb_best, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "print(f\"XGBoost cross-validation RMSE: {-xgb_cv_scores.mean():.5f} (+/- {xgb_cv_scores.std() * 2:.5f})\")\n",
    "\n",
    "# LightGBM cross-validation\n",
    "lgb_cv_scores = cross_val_score(lgb_best, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "print(f\"LightGBM cross-validation RMSE: {-lgb_cv_scores.mean():.5f} (+/- {lgb_cv_scores.std() * 2:.5f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4feafa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set\n",
    "xgb_test_pred = xgb_best.predict(X_test)\n",
    "lgb_test_pred = lgb_best.predict(X_test)\n",
    "\n",
    "# simple ensemble (average predictions)\n",
    "final_pred = (xgb_test_pred + lgb_test_pred) / 2\n",
    "\n",
    "# expm1 transformation to reverse log transformation\n",
    "final_pred = np.expm1(final_pred)\n",
    "\n",
    "# save submission file\n",
    "submission = pd.DataFrame({'Id': test['Id'], 'SalePrice': final_pred})\n",
    "submission.to_csv('submission_baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1725a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define base models for stacking\n",
    "base_models = [\n",
    "    ('rf', RandomForestRegressor(random_state=42)),\n",
    "    ('xgb', XGBRegressor(**xgb_grid.best_params_, random_state=42)),\n",
    "    ('lgb', LGBMRegressor(**lgb_grid.best_params_, random_state=42))\n",
    "]\n",
    "\n",
    "# define meta learner\n",
    "meta_learner = Ridge()\n",
    "\n",
    "# initia Stacking\n",
    "stacking_model = StackingRegressor(estimators=base_models, final_estimator=meta_learner, cv=5)\n",
    "\n",
    "# train Stacking model\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# validate Stacking model\n",
    "stacking_pred = stacking_model.predict(X_val)\n",
    "stacking_rmse = np.sqrt(mean_squared_error(y_val, stacking_pred))\n",
    "print(f\"Stacking RMSE: {stacking_rmse:.5f}\")\n",
    "\n",
    "# cross-validation\n",
    "stacking_cv_scores = cross_val_score(stacking_model, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "print(f\"Stacking cross-validation RMSE: {-stacking_cv_scores.mean():.5f} (+/- {stacking_cv_scores.std() * 2:.5f})\")\n",
    "\n",
    "# predict on test set\n",
    "stacking_test_pred = stacking_model.predict(X_test)\n",
    "stacking_test_pred = np.expm1(stacking_test_pred)\n",
    "\n",
    "# save submission file\n",
    "submission_stacking = pd.DataFrame({'Id': test['Id'], 'SalePrice': stacking_test_pred})\n",
    "submission_stacking.to_csv('submission_stacking.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
